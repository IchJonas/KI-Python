{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainMan:\n",
    "    def __init__(self, path, \n",
    "                 max_depthVal=None, max_depthMax=200, max_depthMin=0, \n",
    "                 min_sample_splitVal = 2, min_sample_splitMax=10, min_sample_splitMin=2, \n",
    "                 clf=None, criterionVal=\"entropy\"):\n",
    "        self.path = path\n",
    "        self.trainList = self.__data()\n",
    "        self.max_depthVal = max_depthVal\n",
    "        self.max_depthMax = max_depthMax\n",
    "        self.max_depthMin=max_depthMin\n",
    "        self.min_sample_splitMax = min_sample_splitMax\n",
    "        self.min_sample_splitMin = min_sample_splitMin\n",
    "        self.min_sample_splitVal = min_sample_splitVal\n",
    "        self.clf = clf\n",
    "        self.criterionVal = criterionVal\n",
    "        self.results = np.empty((3, max_depthMax-max_depthMin, min_sample_splitMax-min_sample_splitMin, 3))\n",
    "    def __data(self):\n",
    "        self.data = pd.read_csv(self.path)\n",
    "        columnsLength = self.data.shape[1]-1\n",
    "        X = self.data.iloc[:,0:columnsLength]\n",
    "        y = self.data[[\"group\"]].values.ravel()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        return [self.X_train, self.X_test, self.y_train, self.y_test]\n",
    "    def get_X_train(self):\n",
    "        \"\"\"Gibt X_train zurück.\"\"\"\n",
    "        return self.X_train\n",
    "    def out(self):\n",
    "        if(self.clf == None):\n",
    "            print(\"Modell ohne Parameter wird angelegt, da keines vorhanden:\")\n",
    "            self.clf = DecisionTreeClassifier(random_state=42)\n",
    "            self.clf = self.clf.fit(self.trainList[0], self.trainList[2])\n",
    "        predict = self.clf.predict(self.trainList[1])\n",
    "        report = classification_report(self.trainList[3], predict, target_names=[\"Atheism\", \"Religion\"])\n",
    "        print(report)\n",
    "        print(\"F-Score: \",f1_score(predict, self.trainList[3], average='macro'))\n",
    "        print(\"Precision: \",precision_score(predict, self.trainList[3], average='macro'))\n",
    "        print(\"Recall: \",recall_score(predict, self.trainList[3], average='macro'))\n",
    "    def max_depth(self, min, max):\n",
    "        self.max_depthMax = max\n",
    "        self.max_depthMin = min\n",
    "        f = 0\n",
    "        for i in range(self.max_depthMax):\n",
    "            if(i<self.max_depthMin):\n",
    "                continue\n",
    "            clf = DecisionTreeClassifier(random_state=42, criterion=self.criterionVal, max_depth=i, min_samples_split=self.min_sample_splitVal)\n",
    "            clf = clf.fit(self.trainList[0], self.trainList[2])\n",
    "            predict = clf.predict(self.trainList[1])\n",
    "            if f1_score(predict, self.trainList[3], average='macro')>f:\n",
    "                f = f1_score(predict, self.trainList[3], average='macro')\n",
    "                self.max_depthVal = i\n",
    "                self.clf =  DecisionTreeClassifier(random_state=42, criterion=self.criterionVal, max_depth=i, min_samples_split=self.min_sample_splitVal)\n",
    "                self.clf = self.clf.fit(self.trainList[0], self.trainList[2])      \n",
    "        return self.max_depthVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden Preprocess dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2624,)\n",
      "(2624,)\n",
      "(656,)\n",
      "(656,)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"preprocessed/dataLemmaLowerStop_train.csv\", on_bad_lines='skip', sep=';')\n",
    "data_test = pd.read_csv(\"preprocessed/dataLemmaLowerStop_test.csv\", on_bad_lines='skip', sep=';')\n",
    "\n",
    "X_train = data_train['text']\n",
    "y_train = data_train['group']\n",
    "X_test =  data_test['text']\n",
    "y_test = data_test['group']\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung data nach Zielklassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsdaten nach Zielklassen aufteilen und in X, y zerlegen\n",
    "dataAth_train = data_train[data_train[\"group\"] == 0]\n",
    "X_Ath_train, y_Ath_train = dataAth_train['text'], dataAth_train['group']\n",
    "\n",
    "dataGraphics_train = data_train[data_train[\"group\"] == 1]\n",
    "X_Graphics_train, y_Graphics_train = dataGraphics_train['text'], dataGraphics_train['group']\n",
    "\n",
    "dataSpace_train = data_train[data_train[\"group\"] == 2]\n",
    "X_Space_train, y_Space_train = dataSpace_train['text'], dataSpace_train['group']\n",
    "\n",
    "dataReli_train = data_train[data_train[\"group\"] == 3]\n",
    "X_Reli_train, y_Reli_train = dataReli_train['text'], dataReli_train['group']\n",
    "\n",
    "# grouped_data_train enthält die DataFrames für jede Gruppe\n",
    "grouped_data_train = [dataAth_train, dataGraphics_train, dataSpace_train, dataReli_train]\n",
    "\n",
    "# Testdaten nach Zielklassen aufteilen und in X, y zerlegen\n",
    "dataAth_test = data_test[data_test[\"group\"] == 0]\n",
    "X_Ath_test, y_Ath_test = dataAth_test['text'], dataAth_test['group']\n",
    "\n",
    "dataGraphics_test = data_test[data_test[\"group\"] == 1]\n",
    "X_Graphics_test, y_Graphics_test = dataGraphics_test['text'], dataGraphics_test['group']\n",
    "\n",
    "dataSpace_test = data_test[data_test[\"group\"] == 2]\n",
    "X_Space_test, y_Space_test = dataSpace_test['text'], dataSpace_test['group']\n",
    "\n",
    "dataReli_test = data_test[data_test[\"group\"] == 3]\n",
    "X_Reli_test, y_Reli_test = dataReli_test['text'], dataReli_test['group']\n",
    "\n",
    "# grouped_data_test enthält die DataFrames für jede Gruppe (Testdaten)\n",
    "grouped_data_test = [dataAth_test, dataGraphics_test, dataSpace_test, dataReli_test]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Liste Wörter welche einen bestimmtem threshold haben erzeugen (für Weiterverarbietung)\n",
    "\n",
    "Problematik:\n",
    "- Diee max_tfidf_scores enhält jeweils den maximalen TF-IDF-Wert eines Wortes spiegelt nur seine Relevanz in einem einzigen Dokument wider, ohne zu berücksichtigen, ob das Wort in anderen Dokumenten relevant ist oder zur Klassifikation beiträgt.\n",
    "=> Lösungs IDEE: Klassen trennen und den Durchschnitsswert nehmen, von den Documenten in denen der TFIDF-Wert nicht null ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "\n",
    "# Funktion zur Berechnung der relevanten Wörter basierend auf TF-IDF\n",
    "def get_top_words(X, threshold):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(X)\n",
    "\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    max_tfidf_scores = tfidf_matrix.max(axis=0).toarray().flatten()\n",
    "\n",
    "    top_words = [features[i] for i in range(len(features)) if max_tfidf_scores[i] > threshold]\n",
    "\n",
    "    return list(set(top_words)), max_tfidf_scores\n",
    "\n",
    "top_words_train, max_tfidf_scores_train = get_top_words(X_train, threshold)\n",
    "\n",
    "\n",
    "print(\"Relevante Wörter für Trainingsdaten:\\n\", top_words_train)\n",
    "print(\"\\nLänge (Train):\\n\", len(top_words_train))\n",
    "print(\"\\ntfidf_scores (Train):\\n\", max_tfidf_scores_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speichrn TFIDF-Matrix in csv\n",
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fidf_matrix_df.to_csv(\"tfidf_matrix.csv\", index=False)\n",
    "#print(\"TF-IDF-Matrix wurde gespeichert.\")\n",
    "#tfidf_matrix_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Liste n-Wörter pro Kategorie mit dem höchsten tfidf Wert erzeugen (für Weiterverarbietung)\n",
    "\n",
    "Weitere Ansätze \n",
    "- Ansatz mit avg_tfidf_scores ausprobieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words_per_category(n=200, grouped_data=None):\n",
    "    # Initialisiere den TfidfVectorizer\n",
    "    #vectorizer = TfidfVectorizer(min_df=1)\n",
    "    vectorizer = TfidfVectorizer(\n",
    "         max_df=0.8,\n",
    "         min_df=2,\n",
    "         ngram_range=(1, 2), #TODO Anpassen für kontext => bisher (1,1) bestes Ergebnis\n",
    "         stop_words='english',\n",
    "         max_features=5000\n",
    "    )\n",
    "\n",
    "    top_words_total = []\n",
    "    top_words_by_group = {}\n",
    "\n",
    "    # Iteriere durch jedes DataFrame in der gruppierten Datenliste\n",
    "    for group_data in grouped_data:\n",
    "        if 'text' not in group_data.columns:\n",
    "            print(\"Textspalte fehlt in den Daten.\")\n",
    "            continue\n",
    "        \n",
    "        group_tfidf_matrix = vectorizer.fit_transform(group_data['text'])  # Berechne TF-IDF-Matrix\n",
    "\n",
    "        features = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        max_tfidf_scores = group_tfidf_matrix.max(axis=0).toarray().flatten()\n",
    "\n",
    "        top_n_indices = max_tfidf_scores.argsort()[-n:][::-1]  # Indices der n höchsten Scores\n",
    "        \n",
    "        top_words_group = [features[i] for i in top_n_indices]\n",
    "        top_words_by_group[group_data['group'].iloc[0]] = top_words_group  # Speichere die Top-Wörter pro Gruppe\n",
    "\n",
    "        top_words_total.extend(top_words_group)\n",
    "\n",
    "    top_words_total = list(set(top_words_total))\n",
    "\n",
    "    return top_words_by_group, top_words_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abspeichern Wörter-Header in CSV (test und train seperat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevante Wörter für Trainingsdaten:\n",
      " ['mechanism', 'centaur', 'mithras', 'time', 'cjf', 'remainder', 'comp', 'graphig', 'version', 'baptize', 'jpg', 'stop', 'motivate', 'abekas', 'xv', 'love', 'breathe', 'pertain', 'car', 'left', 'exhibit', 'stan', 'seven', 'minivas', 'rich', 'renderman', 'paintbrush', 'tyour', 'graphics', 'ascii', 'tup', 'povray', 'pay', 'interrupt', 'camel', 'vmode', 'distance', 'methodology', 'newsgroup', 'sco', 'vpic60', 'servant', 'arm', 'hole', 'hillary', 'prince', 'piper', 'bhagwans', 'wrong', 'group', 'invasion', 'godless', 'ultrix', 'revelation', 'sure', 'positive', 'compile', 'tim', 'wow', 'respect', 'surrender', 'imagine', 'magi', 'weight', 'think', 'ezekiel', 'sei', 'cview', 'want', 'apparent', 'file', 'temperature', 'lucifer', 'color', 'depression', 'ksu', 'gravitational', 'raoul', 'david', 'tto', 'magellan', 'taoism', 'buggy', 'lewb', 'coke', 'usenet', 'rw', 'weitek', 'edge', 'chuck', 'groothuis', 'yo', 'theism', 'trail', 'elf', 'americans', 'presentation', 'jeremy', 'zf', 'keesler', 'help', 'greatly', 'amiga', 'sword', 'cult', 'tyre', 'brain', 'hospital', 'rtrace', 'perijove', 'bottle', 'list', 'wante', 'pbicon', 'face', 'tea', 'tex', 'hindsight', 'iconedit', 'balloon', 'mrs', 'nickname', 'getx11', 'jb', 'leather', 'jest', 'judas', 'propolsion', 'campbell', 'deletion', 'fossil', '2pe', 'km', 'curve', 'spacelab', 'paraphrase', 'train', 'blechhhh', 'tongue', 'benefactor', 'scodal', 'hoi', 'alt', 'internet', 'abortion', 'evolution', 'oh', 'frequently', 'naplps', 'ashamed', 'inbreede', 'couldn', 'vesa', 'option', 'exit', 'rod', 'grandfather', 'splines', 'risk', 'animal', 'benefit', 'resort', 'origins', 'war', 'local', 'jehovah', 'shatim', 'steady', 'cruel', 'carl', 'feynman', 'jw', 'turkey', 'hover', 'sell', 'leary', 'budget', 'project', 'cockroach', 'chest', 'muslims', 'following', 'rocket', 'recent', 'islam', 'parallax', 'globe', 'contradict', 'inaccessible', 'dm', 'satire', 'jpeg', 'iraq', 'galacticentric', 'christian', 'larson', 'leg', 'quit', 'colony', 'gamma', 'choose', 'titan', 'zoroastrians', 'ppmtotga', 'eternal', 'won', 'wwii', 'emblazene', 'ignorance', 'plain', 'rawlin', 'messenger', 'lunacy', 'surname', 'bullshit', 'commodore', 'polygon', 'interested', 'trace', 'clarkson', 'fc', 'negation', 'nope', 'representation', 'tyea', 'corrallary', 'ditto', 'pat', 'atm', 'peace', 'hga', 'ouch', 'banner', 'defend', 'healy', 'precious', 'psa', 'yes', 'green', 'thank', 'incestuous', 'example', 'rh', 'rule', 'believer', 'cost', 'ms', 'toxic', 'hans', 'gb', 'p_c', 'ellipse', 'strength', 'iif', 'classmate', 'emery', 'phigs', 'gifs', 'jimmy', 'shop', 'feel', 'zone', 'worden', 'increase', 'com', 'noonan', 'reveal', 'irit', 'bittrolff', 'description', 'verb', 'special', 'pole', 'compuserve', 'holler', 'zip', 'rectangle', 'language', 'rick', 'bbs', 'grayscale', 'hare', 'targa', 'pub', 'mail', 'door', 'ch', 'island', 'imaginative', 'homosexuality', 'joy', 'crack', 'stereo', 'lewis', 'visuallib', 'horizon', 'curiousity', 'shea', 'log', 'atom', 'excerpt', 'photosynthetic', 'kuiper', '145', 'avi', 'teaching', 'launch', 'million', 'thruster', 'koran', 'dragless', 'gay', 'it', 'lighten', 'prize', 'detail', 'negative', 'trry', 'reply', 'darling', 'detector', 'peter', 'dear', 'tammy', 'find', 'sabbath', 'defect', 'enlighten', 'wording', 'plot', 'hoyle', 'system', 'display', 'acceleration', 'observer', 'behavior', 'claim', 'french', 'prototype', 'gao', 'concrete', 'character', 'model', 'aluminum', 'dv', 'theory', 'pressure', 'shouldn', 'alien', 'ra', 'water', 'plutonium', 'bill', 'ocean', 'figure', 'footage', 'discussion', 'blah', 'ufo', 'doug', 'kc', 'tga', 'tfor', 'shirt', 'reprint', 'childish', 'mythology', 'germany', 'hard', 'response', 'sign', 'compromise', 'access', 'reliable', 'translate', 'radon', 'desease', 'jr0930', 'twho', 'fold', 'copy', 'fr', 'alternative', 'boobtist', 'bd', 'defender', 'racing', 'patch', 'insult', 'union', 'funny', 'jesus', 'block', 'modelling', 'motto', 'heliocentric', 'aerial', 'ipas', 'reliabity', 'omniscient', 'check', 'fasad', 'oto', 'right', 'command', 'keyword', 'corel', 'shark', 'value', 'meal', 'finger', 'tagreed', 'simultaneously', 'mode', 'reality', 'may_93_online', 'appeal', 'grasp', 'bus', 'complex', 'test', 'wp5', 'schedule', 'sirtf', 'hst', 'mohammed', 'glad', 'rb', 'point', 'bcci', 'skywatch', 'contamination', 'nail', 'koan', 'funding', 'kill', 'science', 'tender', 'krs', 'snit', '71', 'nanci', 'text', '24bit', 'macelwaine', 'hoffer', 'deamon', 'lance', 'propagandist', 'gipu', 'crosspost', 'ithaca', 'tibetans', 'eau', 'terrorist', 'streamline', 'karla', 'jews', 'polyhedra', 'word', 'yeager', 'suppose', 'ignorant', 'motor', 'freewill', 'sex', 'late', 'carpenter', 'nonexistence', 'v2', 'sherzer', 'law', 'judgment', 'tails', 'followup', 'religion', 'intersection', 'yourdon', 'validity', 'media', 'jettison', 'sz', 'server', 'regard', 'josephus', 'cm', 'critus', 'otis', 'gravity', 'punishment', 'ti', 'unfortunately', 'salvadorans', 'genoa', 'film', 'sorry', 'ssme', 'bc', 'cain', 'clair', 'projector', 'girl', 'evening', 'tthere', 'info', 'letters', 'depend', 'vms', 'polarity', 'xgif', 'vertex', 'okay', 'pic', 'sunlight', 'jean', 'ole', '000', 'vrrend386', 'odl', 'turtle', 'mary', 'drag', 'painless', 'reuss', 'cross', 'trippy', 'goal', 'bet', 'array', 'negotiate', 'parallelogram', 'chip', 'apology', 'spin', 'wrongly', 'logically', 'identifiable', 'god', 'hijaak', 'sun', 'photon', 'gun', 'rix', 'clouds', 'flight', 'clp', 'coincidence', 'archer', 'keith', 'grow', 'hello', 'hudson', '10', 'malcolm', 'kindergarten', 'image', 'small', 'cover', 'hoopla', 'attention', 'phase', 'penalty', 'crt', 'cop', 'categorizing', 'autodesk', 'generally', 'energy', 'collect', 'offense', 'action', 'signature', 'syllogism', 'export', 'koresh', 'appreciate', 'map', 'proxima', 'build', 'ye', 'liquid', 'cornerstone', 'precedent', 'apple', 'uci', '24', 'dn', 'homosexual', 'whirrr', 'incoming', 'customer', '24th', 'dress', 'cdtv', 'steve', 'wit', 'omnipotent', 'cool', 'avs', 'node', 'rights', 'red', 'stuff', 'cheat', 'elohim', 'qrttoppm', 'advertising', 'dis', 'copyright', 'twhy', 'caution', 'original', 'aren', 'thee', 'cooking', 'ooooo', 'freedom', 'uranus', 'gps', 'texture', 'offensive', 'allah', 'temporary', 'hsv', 'photograph', 'real', 'logo', 'combo', 'agree', 'natch', 'tnot', 'neat', 'ether', 'object', 'moooo', 'add', 'indonesian', 'default', 'ppppp', 'maharishi', 'outline', 'objective', 'postscript', 'ironic', 'urt', 'perpetual', 'bank', 'mind', 'daniel', 'speedstar', 'life', 'software', 'griffin', 'reconcile', 'gregg', 'paul', 'highway', 'surreal', 'beacon', 'philosopher', 'tblessed', 'hacker', 'stage', 'sq', 'mormon', 'class', 'gs', 're', 'idol', 'quran', 'horsepower', 'commandment', 'colour', 'spot', 'mercury', 'br', 'flux', 'question', 'day', 'tresearch', 'seal', 'oort', 'fool', 'faq', 'linux', 'tiff', 'kt', 'explanation', 'somebody', 'answer', 'cosmonautics', 'polaroid', 'rle', 'portuguese', 'macaloon', 'tgee', 'tcould', 'centipede', 'mirror', 'afghans', 'huh', 'jim', 'nsiad', 'umm', 'economical', 'lm', 'bit', 'ekr', 'dispute', 'infinity', 'atheist', 'premium', 'waste', 'srinivas', 'arts', 'shine', 'tand', 'radiosity', 'meng', 'liefting', 'code', 'salvation', 'lambda', 'paradise', 'roehm', 'message', 'spinoff', 'kaveh', 'viewpoint', 'mars', '680x1024', 'employ', 'morality', 'gimme', 'loan', 'graeme', 'equate', 'wisconsin', 'visualization', 'telepathy', 'magnetic', 'subscribe', 'uranium', 'daemon', 'yuan', 'deceive', 'comparable', 'messiah', 'jsn104', 'rastorle', 'tandreas', 'dock', 'pilot', 'moment', 'print', 'delete', 'tiger', 'ics', 'arguer', 'envelope', 'irony', 'conform', 'caste']\n",
      "\n",
      "Länge (Train):\n",
      " 763\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mechanism</th>\n",
       "      <th>centaur</th>\n",
       "      <th>mithras</th>\n",
       "      <th>time</th>\n",
       "      <th>cjf</th>\n",
       "      <th>remainder</th>\n",
       "      <th>comp</th>\n",
       "      <th>graphig</th>\n",
       "      <th>version</th>\n",
       "      <th>baptize</th>\n",
       "      <th>...</th>\n",
       "      <th>print</th>\n",
       "      <th>delete</th>\n",
       "      <th>tiger</th>\n",
       "      <th>ics</th>\n",
       "      <th>arguer</th>\n",
       "      <th>envelope</th>\n",
       "      <th>irony</th>\n",
       "      <th>conform</th>\n",
       "      <th>caste</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 764 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mechanism, centaur, mithras, time, cjf, remainder, comp, graphig, version, baptize, jpg, stop, motivate, abekas, xv, love, breathe, pertain, car, left, exhibit, stan, seven, minivas, rich, renderman, paintbrush, tyour, graphics, ascii, tup, povray, pay, interrupt, camel, vmode, distance, methodology, newsgroup, sco, vpic60, servant, arm, hole, hillary, prince, piper, bhagwans, wrong, group, invasion, godless, ultrix, revelation, sure, positive, compile, tim, wow, respect, surrender, imagine, magi, weight, think, ezekiel, sei, cview, want, apparent, file, temperature, lucifer, color, depression, ksu, gravitational, raoul, david, tto, magellan, taoism, buggy, lewb, coke, usenet, rw, weitek, edge, chuck, groothuis, yo, theism, trail, elf, americans, presentation, jeremy, zf, keesler, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 764 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_by_group_train, top_words_total_train = get_top_n_words_per_category(200, grouped_data_train)\n",
    "\n",
    "print(\"Relevante Wörter für Trainingsdaten:\\n\", top_words_total_train)\n",
    "print(\"\\nLänge (Train):\\n\", len(top_words_total_train))\n",
    "\n",
    "#Abspeicher _train Datei\n",
    "freqList_train = top_words_total_train+[\"group\"]\n",
    "freqListData_train = pd.DataFrame(columns=freqList_train)\n",
    "freqListData_train.to_csv(\"csv/tfidf_mostCommon/20newsgroups_tfidf_200_per_category_train.csv\", index=False)\n",
    "freqListData_train.to_csv(\"csv/tfidf_mostCommon/20newsgroups_tfidf_200_per_category_test.csv\", index=False)\n",
    "freqListData_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wörter Zählen alle (Nur klein schreibung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Daten Häufigkeiten:\n",
      "       mechanism  centaur  mithras  time  cjf  remainder  comp  graphig  \\\n",
      "0             0        0        0     0    0          0     0        0   \n",
      "1             0        0        0     0    0          0     0        0   \n",
      "2             0        0        0     0    0          0     0        0   \n",
      "3             0        0        0     0    0          0     0        0   \n",
      "4             0        0        0     2    0          0     0        0   \n",
      "...         ...      ...      ...   ...  ...        ...   ...      ...   \n",
      "2619          0        0        0     0    0          0     0        0   \n",
      "2620          0        0        0     0    0          0     0        0   \n",
      "2621          0        0        0     1    0          0     0        0   \n",
      "2622          0        0        0     0    0          0     0        0   \n",
      "2623          0        0        0     0    0          0     0        0   \n",
      "\n",
      "      version  baptize  ...  moment  print  delete  tiger  ics  arguer  \\\n",
      "0           0        0  ...       0      0       0      0    0       0   \n",
      "1           0        0  ...       0      0       0      0    0       0   \n",
      "2           0        0  ...       0      0       0      0    0       0   \n",
      "3           0        0  ...       0      0       0      0    0       0   \n",
      "4           0        0  ...       0      0       0      0    0       0   \n",
      "...       ...      ...  ...     ...    ...     ...    ...  ...     ...   \n",
      "2619        0        0  ...       0      0       0      0    0       0   \n",
      "2620        0        0  ...       0      0       1      0    0       0   \n",
      "2621        0        0  ...       0      0       0      0    0       0   \n",
      "2622        0        0  ...       0      0       0      1    0       0   \n",
      "2623        0        0  ...       0      0       0      0    0       0   \n",
      "\n",
      "      envelope  irony  conform  caste  \n",
      "0            0      0        0      0  \n",
      "1            0      0        0      0  \n",
      "2            0      0        0      0  \n",
      "3            0      0        0      0  \n",
      "4            0      0        0      0  \n",
      "...        ...    ...      ...    ...  \n",
      "2619         0      0        0      0  \n",
      "2620         0      0        0      0  \n",
      "2621         0      0        0      0  \n",
      "2622         0      0        0      0  \n",
      "2623         0      0        0      0  \n",
      "\n",
      "[2624 rows x 763 columns]\n",
      "\n",
      "Test-Daten Häufigkeiten:\n",
      "      mechanism  centaur  mithras  time  cjf  remainder  comp  graphig  \\\n",
      "0            0        0        0     0    0          0     0        0   \n",
      "1            0        0        0     0    0          0     0        0   \n",
      "2            0        0        0     0    0          0     0        0   \n",
      "3            0        0        0     0    0          0     0        0   \n",
      "4            0        0        0     2    0          0     0        0   \n",
      "..         ...      ...      ...   ...  ...        ...   ...      ...   \n",
      "651          0        0        0     0    0          0     0        0   \n",
      "652          0        0        0     1    0          0     0        0   \n",
      "653          0        0        0     0    0          0     0        0   \n",
      "654          0        0        0     0    0          0     0        0   \n",
      "655          0        0        0     0    0          0     0        0   \n",
      "\n",
      "     version  baptize  ...  moment  print  delete  tiger  ics  arguer  \\\n",
      "0          0        0  ...       0      0       0      0    0       0   \n",
      "1          0        0  ...       0      0       0      0    0       0   \n",
      "2          0        0  ...       0      0       2      0    0       0   \n",
      "3          0        0  ...       0      0       0      0    0       0   \n",
      "4          3        0  ...       0      0       0      0    0       0   \n",
      "..       ...      ...  ...     ...    ...     ...    ...  ...     ...   \n",
      "651        0        0  ...       0      0       0      0    0       0   \n",
      "652        0        0  ...       0      0       0      0    0       0   \n",
      "653        0        0  ...       1      0       0      0    0       0   \n",
      "654        0        0  ...       0      0       0      0    0       0   \n",
      "655        0        0  ...       0      0       0      0    0       0   \n",
      "\n",
      "     envelope  irony  conform  caste  \n",
      "0           0      0        0      0  \n",
      "1           0      0        0      0  \n",
      "2           0      0        0      0  \n",
      "3           0      0        0      0  \n",
      "4           0      0        0      0  \n",
      "..        ...    ...      ...    ...  \n",
      "651         0      0        0      0  \n",
      "652         0      0        0      0  \n",
      "653         0      0        0      0  \n",
      "654         0      0        0      0  \n",
      "655         0      0        0      0  \n",
      "\n",
      "[656 rows x 763 columns]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Funktion zur Berechnung der Wortfrequenzen und Speicherung in einer CSV\n",
    "def calculate_and_save_word_frequencies(data, freqList, filename):\n",
    "    freqListData_list = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        text = str(row['text'])\n",
    "        group = str(row['group'])\n",
    "        doc = nlp(text)\n",
    "\n",
    "        count_in_freqList = Counter(token.text.lower() for token in doc)\n",
    "\n",
    "        haeufigkeiten = {word: count_in_freqList.get(word.lower(), 0) for word in freqList}\n",
    "        haeufigkeiten['group'] = group\n",
    "\n",
    "        freqListData_list.append(haeufigkeiten)\n",
    "\n",
    "    freqListData = pd.DataFrame(freqListData_list)\n",
    "\n",
    "    freqListData.to_csv(filename, index=False)\n",
    "\n",
    "    return freqListData\n",
    "\n",
    "\n",
    "\n",
    "train_data_file = \"csv/tfidf_mostCommon/20newsgroups_tfidf_200_per_category_train.csv\"\n",
    "freqListData_train = calculate_and_save_word_frequencies(data_train, freqList_train, train_data_file)\n",
    "\n",
    "test_data_file = \"csv/tfidf_mostCommon/20newsgroups_tfidf_200_per_category_test.csv\"\n",
    "freqListData_test = calculate_and_save_word_frequencies(data_test, freqList_train, test_data_file)\n",
    "\n",
    "print(\"Train-Daten Häufigkeiten:\\n\", freqListData_train)\n",
    "print(\"\\nTest-Daten Häufigkeiten:\\n\", freqListData_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLF Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Averaging:\n",
      "F-Score:  0.6002000920941299\n",
      "Precision:  0.6073401235468071\n",
      "Recall:  0.5986424672718854\n",
      "\n",
      "Weighted Averaging:\n",
      "F-Score:  0.6118819717656845\n",
      "Precision:  0.6170700317347014\n",
      "Recall:  0.6128048780487805\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     atheism       0.59      0.50      0.54       155\n",
      "    graphics       0.74      0.67      0.71       190\n",
      "       space       0.58      0.71      0.64       190\n",
      "    religion       0.53      0.51      0.52       121\n",
      "\n",
      "    accuracy                           0.61       656\n",
      "   macro avg       0.61      0.60      0.60       656\n",
      "weighted avg       0.62      0.61      0.61       656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = freqListData_train.drop(\"group\", axis=1)  # Features\n",
    "y_train = freqListData_train[[\"group\"]].values.ravel()  # Zielwerte\n",
    "X_test = freqListData_test.drop(\"group\", axis=1)  # Features\n",
    "y_test = freqListData_test[[\"group\"]].values.ravel()  # Zielwerte\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    random_state=42, \n",
    "    criterion=\"entropy\", \n",
    "    max_depth=130\n",
    ")\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print(\"Macro Averaging:\")\n",
    "print(\"F-Score: \", f1_score(y_test, y_predict, average=\"macro\"))\n",
    "print(\"Precision: \", precision_score(y_test, y_predict, average=\"macro\"))\n",
    "print(\"Recall: \", recall_score(y_test, y_predict, average=\"macro\"))\n",
    "\n",
    "print(\"\\nWeighted Averaging:\")\n",
    "print(\"F-Score: \", f1_score(y_test, y_predict, average=\"weighted\"))\n",
    "print(\"Precision: \", precision_score(y_test, y_predict, average=\"weighted\"))\n",
    "print(\"Recall: \", recall_score(y_test, y_predict, average=\"weighted\"))\n",
    "\n",
    "report = classification_report(y_test, y_predict, target_names=[\"atheism\", \"graphics\", \"space\", \"religion\"])\n",
    "print(\"\\n\"+report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
