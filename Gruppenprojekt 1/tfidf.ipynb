{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re \n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden Preprocess dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preprocessed/dataLower.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessed/dataLower.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocessed/dataLower.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"preprocessed/dataLower.csv\", on_bad_lines='skip', sep=';')\n",
    "data = data.iloc[:,1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroup.csv Datei einlesen und bereinigen nach Index, Duplikaten und Na\n",
    "### Optional (Falls keine PreProcess verwendet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"20 newsgroups/20newsgroups.csv\", on_bad_lines='skip', sep=';')\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()\n",
    "data = data.iloc[:,1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung data nach Zielklassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary für die Gruppen-DataFrames\n",
    "dataAth = data.drop(data[data[\"group\"] != 0 ].index)\n",
    "dataGraphics = data.drop(data[data[\"group\"] != 1].index)\n",
    "dataSpace = data.drop(data[data[\"group\"] != 2].index)\n",
    "dataReli = data.drop(data[data[\"group\"] != 3 ].index)\n",
    "grouped_data = [dataAth, dataGraphics, dataSpace, dataReli]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtern nach Nomen, nur Alpha u. lemmatisierte Wörter\n",
    "#### Optional (Falls keine PreProcess verwendet)\n",
    "\n",
    "Erkentnisse:\n",
    "- Wenn TFIDF anhand von Nomen ermittelt, erhält man mehrere Wörter bei dem gleichen threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade das spaCy-Modell\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Funktion zur Vorverarbeitung des Textes\n",
    "def preprocess_only_nouns(text):\n",
    "    # Text vorverarbeiten: In Kleinbuchstaben umwandeln\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Wende spaCy NLP-Pipeline auf den Text an\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extrahiere nur Nomen (Substantive), lemmatisiere sie und entferne Stoppwörter\n",
    "    lemmatized_tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.pos_ == 'NOUN' and not token.is_stop and token.is_alpha\n",
    "    ]\n",
    "    \n",
    "    # Füge die Tokens wieder zu einem Text zusammen\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess_only_nouns)\n",
    "dataGraphics = dataGraphics['text'].apply(preprocess_only_nouns)\n",
    "dataSpace = dataSpace['text'].apply(preprocess_only_nouns)\n",
    "dataAth = dataAth['text'].apply(preprocess_only_nouns)\n",
    "dataReli = dataReli['text'].apply(preprocess_only_nouns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liste Wörter welche einen bestimmtem threshold haben erzeugen (für Weiterverarbietung)\n",
    "\n",
    "Problematik:\n",
    "- Diee max_tfidf_scores enhält jeweils den maximalen TF-IDF-Wert eines Wortes spiegelt nur seine Relevanz in einem einzigen Dokument wider, ohne zu berücksichtigen, ob das Wort in anderen Dokumenten relevant ist oder zur Klassifikation beiträgt.\n",
    "=> Lösungs IDEE: Klassen trennen und den Durchschnitsswert nehmen, von den Documenten in denen der TFIDF-Wert nicht null ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevante Wörter:\n",
      " ['rgb', 'afghans', 'sunlight', 'muller', 'sigkids', 'door', 'imagine', 'nudists', 'gifs', 'groothuis', 'concrete', 'year', 'vms', 'keith', 'increase', 'postscript', 'cx5', 'v2', 'acceleration', 'dock', 'tagreed', 'rayshade', 'spinoffs', 'atm', 'confused', '_email_', 'with', 'eternal', 'map', 'naplps', 'cockroaches', 'planets', 'islam', 'quote', 'van', 'dm', 'iff', '_rejects_', 'ra', 'image', 'mars', 'auggestions', 'hga', 'tnot', 'rendering', 'arm', 'tshe', 'weitek', 'sounds', 'commandment', 'p3', 'ocean', 'bullshit', 'bet', 'amongst', 'leather', 'tolerances', 'mpeg_play', 'deseases', 'radiosity', 'ramjet', 'hasn', 'twhy', 'blood', 'malcolm', '_simple', 'hacker', 'pens', 'pub', 'thanks', 'baptize', 'vmode', 'curve', 'penalty', 'sherzer', 'heliocentric', 'special', 'comparable', 'rastorle', 'tiger', 'incestuous', 'card', 'gps', 'temperature', 'angular', 'called', '________________________________________________________________________', 'jupiter', 'star', 'raoul', 'talking', 'wants', 'goal', 'doug', 'dragless', 'bcci', 'forces', 'tyea', 'origins', 'ether', 'dna', 'bhagwans', 'he', '000', 'positional', 'psa', 'enlightening', 'punishment', 'sound', 'carl', 'noticed', 'galacticentric', 'convenient', 'apl', 'fill', 'tamu', 'propolsion', 'strikes', 'faq', 'morally', 'imperial', 'stuff', 'graeme', 'trace', 'sold', 'emblazened', 'discusses', 'format', '24bit', 'fuselage', 'toak', 'ahhh', 'gamma', 'lcd', 'million', 'sunset', 'atoms', 'amiga', 'chip', 'chest', 'ground', 'tpersonally', 'indirect', 'wow', 'thee', 'words', '3do', 'routine', 'block', '_defined_', 'because', 'speedstar', 'compromise', 'search', 'triangulized', 'projections', 'drivers', 'yuan', 'toll', 'shatim', 'alice', 'bates', 'abekas', 'contradiction', 'pbicon', '24', 'carpenter', 'floppy', 'name', 'screen', 'copyrighted', 'shadows', 'inbreeding', 'shape', 'defense', 'voyager', 'mirage', 'smoking', 'pregnancy', 'lambda', 'vga', 'tany', 'claim', 'critus', 'rules', 'enamoured', 'ms', 'polarity', 'qumram', 'snit', 'mrs', 'specs', 'kc', 'red', 'precompiled', 'hussein', 'him', 'extended', 'program', 'hillary', 'gravitational', 'contradicting', 'points', 'epsi', 'capacitor', 'centaur', 'shirts', 'speech_', 'infinity', 'psychological', 'visuallib', 'washed', 'code', 'defer', 'theism', 'gay', 'proving', 'bobby', 'arbitron', 'violation', 'simultaneously', 'bull', 'schirra', 'tyour', 'wives', 'why', 'cult', 'arizona', 'globe', 'speak', 'crossposting', 'centipede', 'patch', 'zn', 'autotheism', 'modelers', 'wwii', 'koff', 'neat', 'lars', 'news', 'beast', 'face', 'atheist', 'positive', 'accelerations', 'jb', 'foley', 'thave', 'tandreas', 'interrupt', 'umm', 'contact', 'lsd', 'parallax', 'trry', 'utexas', '24th', 'compartmentalizing', 'mormon', 'corel', 'miro', 'buckminster', 'vrrend386', 'observer', 'island', 'falsehoods', 'philosopher', 'maharishi', 'caste', 'prometheus', 'validity', 'sz', 'moon', 'the', 'reliable', 'viewer', 'wp5', 'puddle', 'apparent', 'twho', 'suppose', 'entity', 'subscribe', 'ironic', 'iraq', 'animals', 'blashephemers', 'cosmonautics', 'lib', 'cornerstone', 'behaviors', 'unofficial', 'adultery', 'lemur', 'stamp', 'controller', 'stacks', 'wingert', 'trillion', 'qrttoppm', 'sabin', 'sq', 'her', 'csd', 'aleph', 'shining', 'contacting', 'trinity', 'encoders', 'photosynthetic', 'mithras', 'imaginative', 'allah', 'satire', 'prototype', 'daemon', 'iconedit', 'windows', 'abandon', 'sandvik', 'stephen', 'equating', 'ezekiel', 'negotiators', 'piper', 'catholic', 'appreciate', '8287', 'characters', 'tnice', 'recycled', 'precedent', 'cosmologist', 'lehman', 'com', 'pcs', 'responses', '3d', '45g', 'verb', 'banner', 'lucas', 'contadictions', 'ppppp', 'theory', 'clarkson', 'lies', 'graphigs', '___', 'dr', 'get', 'arrays', 'basalts', 'funny', 'formats', 'br', 'tblessed', 'roehm', 'meng', 'come', '_that_', 'project', 'fossil', 'car', 'pink', '2pe', 'fr', 'genoa', 'morality', 'exit', 'jeremy', 'barnsley', 'add', 'dadies', 'interpolation', 'someone', 'usgs', 'karla', 'camel', 'prizes', 'omniscient', 'village', 'shading', 'worden', 'raj', 'cops', 'perth', 'ssmes', 'vell', '1970', 'objects', 'signature', 'hello', 'aliens', 'stunningly', 'left', 'pasta', 't0', 'rainer', 'st6', 'vesa', 'jw', 'soul', 'syntax', 'reconcile', 'negation', 'koran', 'love', 'qcr', 'keesler', 'lantern', 'bothering', 'steady', 'stan', '680x1024', 'may_93_online', 'surreal', 'mom', 'wally', 'childish', 'proxima', 'scott', '68010', 'jest', 'hijaak', 'radon', 'kaman', 'b12', 'nonexistence', 'rigel', 'hoped', 'restrictions', 'can', 'wife', 'ye', 'courtroom', 'surname', 'precious', 'info', 'joe', 'voronoi', 'macs', 'tiff', 'surfaces', 'mono', 'hangeth', 'gehrels', 'gimme', 'yourself', 'sure', 'coca', 'offense', 'meshing', 'mohammed', 'whatever', 'hindus', 'rat', 'behaviour', 'hoban', 'idea', 'widget', 'incoming', 'morioka', 'naren', 'advertising', 'cyclical', 'genocide', 'woods', 'vor', 'paintbrush', 'footage', 'converter', 'brilliant', 'followups', 'algorithm', 'supported', '15mhz', '_talk_', 'uranium', 'depression', 'wit', 'facts', 'offline', 'cview', 'planes', 'splines', 'breathing', 'solved', 'fc', 'oh', 'tossed', 'curiousity', 'uranus', 'kibology', 'rod', 'sgi', 'newsgroup', 'animated', 'tand', 'targa', 'apples', 'cjf', 'perhaps', 'jews', 'gota', 'viewpoint', 'um', 'clip', 'pixel', 'wanted', 'theories', 'beem', 'bookstore', 'ove', 'conner', 'irit', 'steve', 'yo', 'tyre', 'film', 'media', 'language', 'nsiad', 'tokyo', 'codification', 'faces', 'dumas', 'otis', 'tcomp', 'dis', 'copyrights', 'landing', 'courts', 'ribbons', 'elves', 'edge', 'stop', 'flux', 'pd', 'respect', 'anarchist', 'tmp', 'looks', 'pc386', 'tpublications', 'americans', '_the_transcedental_temptation_', 'highway', 'op_rows', 'super', 'salvation', 'lynne', 'sabbath', 'telepathy', 'test', 'homosexuality', 'bible', 'sirtf', 'stage', 'sunrise', 'jimmy', 'exposing', 'trimming', 'stead', 'minivas', 'laws', 'zone', 'belief', 'doubtless', 'sail', 'songs', 'personnel', 'methodology', 'colony', 'temporarily', 'temporary', 'bittrolff', 'launch', 'cxourt', 'magnetic', 'legs', 'access', 'freewill', 'trippy', 'opted', 'tibetans', 'paradise', 'polaroid', 'healy', '100', 'rle', 'cooking', 'perpetual', 'oto', 'zip', 'hole', 'dolphins', 'ditto', 'varieties', 'huh', 'generally', 'hover', 'bit', 'pex', 'unions', 'kaveh', 'dang', 'jsn104', 'virtual', 'flights', 'msdos', 'ascii', 'observations', 'sex', 'salvadorans', 'xviewgl_v1', 'cain', 'pope', 'turtle', 'out', 'sphinx', 'arts', 'heavier', 'hudson', 'ellipse', 'figures', 'joke', 'mirror', 'liquid', 'yes', 'amorc', 'alt', 'processing', 'vonnegut', 'space', 'freedom', 'madmen', 'da', 'please', 'anybody', 'ipas', 'alaising', 'motivated', 'ole', 'ouch', 'tfor', 'crossed', 'rights', 'yeager', 'unb', 'tons', 'iroquois', 'mercury', 'find', 'jesus', 'nine', 'combo', 'op_cols', 'dress', 'shi', 'animation', 'drag', 'numbers', 'lm', 'bounded', 'answered', 'jr0930', 'fonts', 'paraphrase', 'tgee', 'racing', 'rosicrucian', 'logically', 'bear', 'tspl', 'running', 'spaceplane', 'tto', '____________________________________________________', 'shea', 'absolute', 'keywords', 'luminosity', 'scheduled', 'rh', 'skywatch', 'aerial', 'greek', 'hans', 'tthere', 'upstanding', 'zillion', 'p2', 'p_c', 'founded', 'revealed', 'horsepower', 'playback', 'polygon', 'oort', 'object', 'griffin', 'terrorist', 'blah', 'spacelab', 'resource', 'feynman', 'color', 'changed', 'agree', 'xviewgl', 'tspread', 'xpresso', 'syndication', 'ti', 'tubes', 'astounded', 'yawn', 'caligiuri', 'arms', 'worlds', 'darwinners', 'phigs', '71', 'translate', 'hindsight', 'cheat', 'jean', 'lightened', 'strength', 'loans', 'odl', 'letters', 'renderman', 'wrongly', 'iig', 'compiler', 'mat26', 'lewb', 'kt', 'urt', 'neie', 'ramjets', 'pgm', 'quit', 'rocketry', 'hangings', 'gl', 'wording', 'plane', 'bank', 'charon', 'bands', 'images', 'irony', 'tresearch', 'acns', 'markc', 'indonesian', 'garden', 'revelation', 'zoroastrians', 'dv', 'streamline', 'corrallary', 'fasad', 'if', 'returns', 'intersections', 'chrisj', 'forty', 'demo', 'convinced', '_______________________________________________________________________________', 'benefits', 'compuserve', 'list', 'pond', 'humanist', 'log', 'preferably', 'wingate', 'exhibit', 'software', 'paying', 'taoism', 'spot', 'reuss', 'finnish', 'gillow', 'polygons', 'me', 'predicate', 'linux', 'coded', 'ruin', 'nerva', 'contradictory', 'tim', 'maddi', 'ooooo', 'fingers', 'volunteering', 'zyda', 'sine', 'firsthand', 'mercedes', 'believer', 'copped', 'number', 'balloon', 'comrades', 'blechhhh', 'surrender', 'enterprises', 'station', 'broken', 'ing', 'deleted', 'quest', 'liquids', 'nope', 'graphics', 'vogon', 'tornadoes', 'most', 'de', 'reader', 'motors', 'darling', 'ashamed', 'magi', 'connotations', 'tdahlgren', 'elohim', 'quadric', 'smiley', 'visualization', 'stu', 'categorizing', 'temp', 'secular', 'gipu', 'hst', 'aliasing', 'spacecraft', 'sufism', 'enviroleague', 'rw', 'outline', 'coerce', 'billboard', 'tunless', 'law', 'tscott', 'mythology', 'tlistserv', 'six', 'talk', 'grow', 'ati', 'greatly', 'answer', 'motto', 'fifty', 'files', 'coplanar', 'colonies', 'mozumder', 'jettison', 'phase', 'quartic', 'ppmtotga', 'sits', 'normals', 'jpeg', 'rich', 'archer', 'prince', 'envelope', 'xgif', 'sei', 'premium', 'alias', 'replied', 'server', 'you', 'trashing', 'spelled', 'earthquake', 'gao', 'moral', 'tammy', 'syllogism', 'somebody', 'energy', '__', 'prefereably', 'displays', 'gun', 'static', 'scodl', 'where', 'ekr', 'water', 'firings', 'teachings', 'lucifer', 'quran', 'register', 'em', 'peace', 'keys', 'prize', 'messenger', 'gb', 'euclidean', 'evening', 'hsv', 'emery', 'david', 'ultrix', 'uninformed', 'clp', 'insult', 'uci', 'god', 'recent', 'dispite', 'bullets', 'texts', 'targas', 'gs', 'macaloon', 'memory', 'lunar', 'dissatisfactions', '377', 'economical', 'holler', '24x', 'spam', 'directories', 'whirr', 'tcould', 'wisconsin', 'hoi', 'alternative', 'yourdon', 'painless', 'conform', 'pressure', 'print', 'zf', 'buggy', 'crack', 'green', 'fw', 'photographs', 'wwhite', 'french', 'liefting', 'whats', 'gas', 'jpg', 'eau', 'grand', 'cylinder', 'xv', 'scanner', 'plutonium', 'nyikos', 'slater', 'opens', 'bc', 'helix', 'achilles', 'coutesy', 'cola', 'sexium', 'den', 'pmdb', 'cdtv', 'objective', 'koan', 'social', 'hell', 'josephus', 'cruel', 'hee', 'germany', 'printer', 'life', 'comp', 'routines', 'cd', 'dn', 'benefactor', 'parsing', 'lindley', 'attention', 'comet', 'values', 'worse', '0m', 'grasp', 'povray', 'xloadimage', 'abortion', 'religion', 'bill', 'run', 'stretcheth', 'frequently', 'nanci', 'lazarus', 'complex', 'messiah', 'iges', 'morals', 'ksu', 'teflon', 'pilot', 'choose', '_perijoves_', 'lance', 'reality', 'commodore', 'vertices', 'techbook', 'mm', 'banks', 'shop', 'word', 'fractal', 'campbell', 'feed', 'miller', 'appologies', 'islamic', 'breath', 'mining', 'message', 'idol', 'aluminum', 'reprints', 'exe', 'identifiable', 'char', 'polyhedra', 'dorsai', 'command', 'cyberware', 'promises', 'ignorant', 'galileo', 'we', 'convicted', 'engines', '68070', 'tga', 'leary', 'launcher', 'getx11', 'jehovah', '72105', 'osiris', 'larson', 'what', '________________________________________________________________________________', 'grandfather', 'resorts', 'cool', 'italians', 'developable', 'distance', 'olympus', 'customers', 'thug', 'pixar', 'admits', 'ics', 'war', 'ites', 'designation', 'hoyle', 'twenty', 'ai', 'keresh', 'jim', 'playmation', 'quantization', 'nah', 'file', 'tea', 'copy', 'citizenship', 'internet', 'projector', 'classmate', 'toxic', 'gregg', 'kurtz', 'brian', 'kindergarten', 'vlbi', 'girl', 'thou', 'koresh', 'hare', 'shell', 'fools', 'tex', 'made', 'generated', 'real', 'stand', 'indeed', '1351', 'worshipping', 'thruster', 'blame', 'bbs', 'ocr104', 'grafsys', 'hoffer', 'freon', 'tongue', 'pratical', 'mp', 'decoders', 'deletion', 'chuck', 'traw', 'books', 'titan', 'frustrating', 'bus', 'excerpt', 'horizon', 'krzysztof', 'italy', 'turkey', 'joy', 'train', 'ignorance', 'stratavision', '8514', 'simulation', 'shark', 'unfortunately', 'mach', 'ntsc', 'madonna', 'ssme', 'haston', 'ks', 'classes', 'vax1', 'contamination', 'macelwaine', 'russians', 'acid', 'beautiful', 'needed', 'propagandist', 'sco', 'scodal', 'mormons', 'ufos', 'hardware', 'godless', 'magellan', 'big', 'parallelogram', 'bloated', 'stamps', 'wrong', 'amen', 'sign', 'update', 'portuguese', 'apologies', 'autodesk', 'peter', 'contacts', 'srinivas', 'permit', 'xxxx', 'crosspost', 'dos', 'szabo', 'emx', 'grayscale', 'arguers', 'clementine', 'lunacy', 'gammas', 'rix', 'cocaine', 'pics', 'cover', 'nd', 'correction', 'equal', 'itself', 'hp2xx', 'pissed', 'local', 'tadaptive', 'columbus', 'rectangle', 'cheaper', 'dta15', 'iif', 'laserwriter', 'gopher', 'ithaca', 'funding', 'clair', 'opposing', 'vpic60', 'hdf', 'cameo', 'muslims', 'flourish', 'irt', 'standards', 'tup', 'colours', 'seals', 'coke', 'sequel', 'putt', 'mercies', 'ch', 'indo', 'txli', 'atheism', 'pat', 'natch', 'regard', 'lewis', 'mire', 'xyzzy', 'compress', 'whirrr', 'palm', 'tails', 'challenges', 'goldin', 'nails', 'noonan', 'aldrin', 'beauty', 'krs', 'evolution', 'adobe', 'crt', 'rb', 'bd', 'default', 'moments', 'moooo', 'goals', 'avs', 'daniel', 'auckland', 'omnipotent', 'rtrace']\n",
      "\n",
      "Länge:\n",
      " 1271\n",
      "\n",
      "tfidf_scores:\n",
      " [0.31373715 0.69071476 0.09788128 ... 0.03587717 0.41765143 0.08568859]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.4\n",
    "\n",
    "# Berechne die TF-IDF-Werte\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Hole die Wortliste (Feature-Namen), keine duplikate\n",
    "features = vectorizer.get_feature_names_out()\n",
    "tfidf_matrix_df = pd.DataFrame(tfidf_matrix.toarray(), columns=features)\n",
    "\n",
    "# Extrahiere die maximalen TF-IDF-Werte für jedes Wort\n",
    "max_tfidf_scores = tfidf_matrix.max(axis=0).toarray().flatten()\n",
    "\n",
    "# Filtere Wörter, deren TF-IDF-Wert den Schwellenwert überschreiten\n",
    "tfidf_relevant_words = [features[i] for i in range(len(features)) if max_tfidf_scores[i] > threshold]\n",
    "\n",
    "# Gib die relevanten Wörter aus\n",
    "tfidf_relevant_words = list(set(tfidf_relevant_words))\n",
    "print(\"Relevante Wörter:\\n\", tfidf_relevant_words)\n",
    "print(\"\\nLänge:\\n\", len(tfidf_relevant_words))\n",
    "print(\"\\ntfidf_scores:\\n\", max_tfidf_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test unterschied Ergebnis relevanteste wörter und relevanteste Nomen vergleich F Wert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speichrn TFIDF-Matrix in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtfidf_matrix_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtfidf_matrix.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF-Matrix wurde gespeichert.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m tfidf_matrix_df\u001b[38;5;241m.\u001b[39mhead\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/formats/csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/formats/csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/formats/csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/pandas/io/formats/csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[1;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[0;32m--> 324\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mwriters.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf_matrix_df.to_csv(\"tfidf_matrix.csv\", index=False)\n",
    "print(\"TF-IDF-Matrix wurde gespeichert.\")\n",
    "tfidf_matrix_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liste n-Wörter pro Kategorie mit dem höchsten tfidf Wert erzeugen (für Weiterverarbietung)\n",
    "\n",
    "Weitere Ansätze \n",
    "- Ansatz mit avg_tfidf_scores ausprobieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.07430544e-05 1.79506788e-03 1.40954830e-04 ... 1.33462355e-04\n",
      " 5.26707546e-05 2.83761287e-04]\n",
      "[2.79997697e-03 1.04647523e-03 7.62626796e-06 ... 1.05078985e-05\n",
      " 4.00014567e-04 2.10241626e-04]\n",
      "[2.08297565e-03 2.83225457e-03 2.03942853e-04 ... 1.78051008e-05\n",
      " 1.78051008e-05 3.56102017e-05]\n",
      "[7.00768892e-05 8.76285921e-04 1.40709352e-04 ... 1.43762817e-05\n",
      " 2.12377260e-04 4.02297981e-05]\n",
      "Top TF-IDF-Wörter pro Gruppe:\n",
      "Gruppe 0: the, of, to, is, that, and, you, it, in, not ...\n",
      "Gruppe 1: the, to, of, and, it, is, for, in, you, that ...\n",
      "Gruppe 2: the, to, of, and, is, in, it, that, you, for ...\n",
      "Gruppe 3: the, to, of, that, and, is, you, in, it, not ...\n",
      "\n",
      "Alle einzigartigen Top-Wörter (gesamt):\n",
      "vga, dxf, off, this, video, through, line, statements, polygons, directory, development, rushdie, same, package, jim, word, that, islamic, algorithms, cause, technology, send, modes, see, help, does, even, moon, display, says, control, thus, care, world, again, gov, shareware, buy, orbital, have, working, them, started, earth, understand, society, vice, statement, rather, define, of, ground, made, before, matter, done, our, format, case, knows, believe, problems, stay, quote, first, ac, having, solar, jesus, is, created, bank, view, office, exists, version, xv, dos, satellite, star, data, from, it, some, memory, works, whether, morals, go, accept, non, called, high, russian, mary, sci, qur, by, both, sin, sort, pub, most, space, kind, thanks, vesa, christ, get, true, write, child, heaven, man, big, though, hst, all, must, 10, years, comp, re, colors, cult, field, map, set, sank, was, quite, blew, book, second, system, hi, ray, got, sun, next, had, please, note, time, tyre, interested, social, claim, area, rest, programming, place, died, mr, should, point, sea, agree, him, bobby, times, check, several, each, routine, tried, phigs, now, nature, posting, sounds, blood, only, screen, maybe, make, ye, ll, said, after, thing, christian, military, info, general, follow, post, mission, peace, whole, able, found, whatever, last, launched, mass, within, end, certain, draw, temperature, mail, rocket, great, men, church, malcolm, kent, because, let, ever, dc, bob, written, object, they, never, code, theists, were, three, exist, always, logic, reason, for, against, false, ago, there, 16, guess, work, quality, night, around, text, the, an, 1993, know, need, fine, example, 3do, propulsion, my, center, interpretation, 30, small, fire, in, computer, deleted, fast, level, unix, he, adobe, seen, these, me, 256, haven, kill, commercial, creation, long, two, old, but, station, going, into, personal, god, definition, truth, ve, theory, jews, we, what, others, funding, author, thank, cost, tells, wouldn, jpl, her, want, words, advertising, aren, pictures, therefore, net, missions, and, us, flight, as, site, heard, application, different, colour, few, day, which, getting, wondering, possible, design, file, vehicle, tand, better, argument, you, animals, gamma, mode, bill, million, without, known, based, hope, sgi, between, number, people, dead, may, pluto, your, available, ftp, cannot, their, group, mac, large, away, something, part, project, interesting, above, low, reality, lord, wrong, sex, she, proof, greatly, safety, freedom, sorry, discuss, sources, religious, laws, moment, under, cview, law, give, muslim, color, astronomy, nasa, costs, formats, come, anybody, drivers, shall, michael, address, life, test, question, at, built, theism, own, evil, motto, edu, doing, did, put, where, opinion, 00, tiff, routines, won, subject, near, take, amiga, currently, another, given, cobb, reference, start, cd, reading, with, newsgroup, being, if, perfect, far, position, queens, satellites, 20, hello, feel, least, prize, problem, ms, process, killed, war, via, uk, course, sites, enough, history, useful, talk, money, cruel, surface, try, saying, hell, pc, used, so, good, tell, year, atheism, yourself, out, images, about, live, didn, arguments, strong, energy, driver, bmp, remember, up, too, information, could, april, been, pretty, children, new, belief, picture, less, today, fact, look, why, research, state, landing, don, windows, running, very, shuttle, down, mean, satan, beauchaine, than, looking, free, keep, fbi, themselves, svga, yet, air, doesn, any, self, since, no, viewer, probably, john, tek, tga, become, religions, read, when, graphic, oort, according, are, person, files, back, name, faith, using, nothing, everyone, machine, tom, image, keith, public, during, taking, current, do, pov, death, pixel, science, every, while, or, atmosphere, until, turkey, 000, hand, christianity, lunar, on, launch, create, ico, natural, company, find, points, exactly, christians, clear, planet, thou, human, yes, etc, making, light, posts, programs, use, like, bible, rights, moral, atheists, followers, run, more, original, has, list, jpeg, assume, perhaps, bad, gif, 3d, hear, abortion, correct, appreciated, mars, deletion, talking, systems, rules, show, nice, spacecraft, per, support, sure, himself, dryden, paradise, order, thought, rockets, humans, else, either, converter, manhattan, animation, explain, his, library, zip, isn, just, output, ti, context, well, can, stuff, its, processing, little, polygon, type, koresh, card, alt, real, program, access, here, atheist, faq, ames, unfortunately, source, news, conversion, jupiter, things, hard, brian, once, also, days, over, ibm, comes, someone, value, software, algorithm, cheers, objects, article, right, actually, ask, those, siggraph, mormon, simply, allen, trying, shafer, idea, lines, david, served, government, religion, think, email, rgb, bronx, bus, punishment, am, bits, die, seems, rendering, postscript, boost, fax, will, really, issue, beliefs, much, not, how, com, fuel, muslims, be, claims, bobbe, copy, seem, way, books, following, other, might, however, story, sky, graphics, answer, means, gods, hardware, university, evidence, morality, phone, anyone, already, to, 50, islam, best, consider, lot, then, anyway, cards, who, 24, pcx, certainly, makes, mind, rosicrucian, provide, cheaper, sense, advance, itself, one, love, billion, call, anything, still, say, pat, would, standard, values, plane, such, orbit, convert, everything, bit, oh, simple, existence, command, objective, mormons, many, power\n",
      "Anzahl aller einzigartigen Wörter: 777\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_words_per_category(n=400):\n",
    "    # Initialisiere den TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "\n",
    "    # Liste um die Top n Wörter zu speichern\n",
    "    top_words_total = []\n",
    "    top_words_by_group = {}\n",
    "\n",
    "    for group_data in grouped_data:\n",
    "\n",
    "\n",
    "        group_tfidf_matrix = vectorizer.fit_transform(group_data['text']) #TODO nicht so effezient\n",
    "\n",
    "\n",
    "        # Hole die Wortliste (Feature-Namen)\n",
    "        features = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # TODO OPT Berechne den Durchschnittlichen TF-IDF-Wert pro Wort für diese Kategorie, ggfs erweitern mit rausfiltern der Nullen \n",
    "        # avg_tfidf_scores = group_tfidf_matrix.mean(axis=0).A.flatten() #TODO: Testen mit 0 Zeilen da eigentlich gute Kombination aus tfidf und count => schlecht nur Wörter wie the etc.\n",
    "\n",
    "\n",
    "        # Extrahiere die maximalen TF-IDF-Werte für jedes Wort\n",
    "        # max_tfidf_scores = group_tfidf_matrix.max(axis=0).toarray().flatten()\n",
    "        print(max_tfidf_scores)\n",
    "        \n",
    "        # Extrahiere die Wörter mit den höchsten TF-IDF-Werten (Top n)\n",
    "        top_n_indices = max_tfidf_scores.argsort()[-n:][::-1]  # Die Indices der n höchsten Scores\n",
    "        \n",
    "        top_words_group = [features[i] for i in top_n_indices]\n",
    "        top_words_by_group[group_data['group'].iloc[0]] = top_words_group\n",
    "\n",
    "        # Speichere die Top n Wörter in der Liste\n",
    "        top_words_total.extend(top_words_group)\n",
    "    \n",
    "    # Füge alle relevanten Wörter zusammen (optional, falls du sie vereinen möchtest)\n",
    "    top_words_total = list(set(top_words_total))\n",
    "\n",
    "    return top_words_by_group, top_words_total\n",
    "\n",
    "\n",
    "# Aufruf der Funktion\n",
    "top_words_by_group, top_words_total = get_top_n_words_per_category()\n",
    "\n",
    "# Ausgabe der Top-Wörter pro Gruppe\n",
    "print(\"Top TF-IDF-Wörter pro Gruppe:\")\n",
    "for group, words in top_words_by_group.items():\n",
    "    print(f\"Gruppe {group}: {', '.join(words[:10])} ...\")  # Ausgabe der ersten 10 Wörter pro Gruppe\n",
    "\n",
    "# Ausgabe aller einzigartigen Top-Wörter\n",
    "print(\"\\nAlle einzigartigen Top-Wörter (gesamt):\")\n",
    "print(\", \".join(top_words_total))\n",
    "print(f\"Anzahl aller einzigartigen Wörter: {len(top_words_total)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Classifier mit Vektor trainieren\n",
    "\n",
    "TODOs:\n",
    "- Ggfs. autotrainer für TFID werte schreiben, bzw. wie ermittle ich die bessten werte\n",
    "- Ggfs. Prüfen wie bei Vektoren Klassen einzeln behandeln\n",
    "- Weitere Ansätze mit Lower etc.u. Nomen, ggfs einfach mit den Dateien von Moritz \n",
    "- Trennung Vectoren nach Group (siehe Chat gpt)\n",
    "- Weitere Parameter testen\n",
    "    tfidf = TfidfVectorizer(\n",
    "    max_df=0.8,               # Häufige Wörter, die mehr als in 80% der Dokumente vorkommen, werden ignoriert\n",
    "    min_df=2,                 # Nur Wörter, die in mindestens 2 Dokumenten vorkommen\n",
    "    ngram_range=(1, 2),       # Unigramme und Bigramme\n",
    "    stop_words='english',     # Entfernt englische Stoppwörter\n",
    "    max_features=5000,        # Maximale Anzahl von Features\n",
    "    use_idf=True,             # Verwende die inverse Dokumenthäufigkeit\n",
    "    sublinear_tf=True         # Logarithmische Transformation der Termfrequenzen\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dominik/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "180 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dominik/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/dominik/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/dominik/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/home/dominik/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/dominik/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/dominik/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/dominik/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59055907 0.58097418 0.60632591 0.59355684 0.59055907 0.58097418\n",
      " 0.60632591 0.59355684 0.59055907 0.58097418 0.60632591 0.59355684\n",
      " 0.5889635  0.578599   0.59961138 0.59457898 0.5889635  0.578599\n",
      " 0.59961138 0.59457898 0.5889635  0.578599   0.59961138 0.59457898\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.6041278  0.58349946 0.61000615 0.59791611 0.6041278  0.58349946\n",
      " 0.61000615 0.59791611 0.6041278  0.58349946 0.61000615 0.59791611\n",
      " 0.60263647 0.58719238 0.60672804 0.60036565 0.60263647 0.58719238\n",
      " 0.60672804 0.60036565 0.60263647 0.58719238 0.60672804 0.60036565\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60034111 0.584368   0.61038088 0.59733162 0.60034111 0.584368\n",
      " 0.61038088 0.59733162 0.60034111 0.584368   0.61038088 0.59733162\n",
      " 0.6014844  0.58804276 0.61452449 0.60164102 0.6014844  0.58804276\n",
      " 0.61452449 0.60164102 0.6014844  0.58804276 0.61452449 0.60164102]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'clf__max_depth': 130, 'clf__min_samples_split': 4, 'tfidf__max_df': 0.4, 'tfidf__max_features': 5000, 'tfidf__min_df': 1}\n",
      "F1-Score:  0.6344803661734597\n",
      "Precision:  0.6390127402416347\n",
      "Recall:  0.6324285453703574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.52      0.52       155\n",
      "           1       0.80      0.76      0.78       190\n",
      "           2       0.65      0.73      0.69       190\n",
      "           3       0.58      0.51      0.54       121\n",
      "\n",
      "    accuracy                           0.65       656\n",
      "   macro avg       0.64      0.63      0.63       656\n",
      "weighted avg       0.65      0.65      0.65       656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Daten aufteilen\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'], data['group'], test_size=0.2, random_state=42, stratify=data['group']\n",
    ")\n",
    "\n",
    "# Pipeline definieren\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_df=0.8,\n",
    "        min_df=2,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        max_features=5000\n",
    "    )),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter-Suchraum definieren\n",
    "param_grid = {\n",
    "    'tfidf__max_df': [0.4, 0.5, 0.6 ],          # Reduziere auf zwei sinnvolle Werte\n",
    "    'tfidf__min_df': [1,3],             # Wesentlich: 1 (selten), 10 (mittlere Häufigkeit)\n",
    "    'tfidf__max_features': [500, 5000],   # Fokus auf kleinere Werte für Geschwindigkeit\n",
    "    'clf__max_depth': [70, 100, 130],    # Relevante Werte für Entscheidungsbaumtiefe\n",
    "    'clf__min_samples_split': [1, 2, 4]     # Kleinere, repräsentative Auswahl\n",
    "}\n",
    "\n",
    "# GridSearchCV mit F1-Score als Bewertungsmaß\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                          # 5-fache Cross-Validation\n",
    "    scoring='f1_macro',            # Optimierung auf F1-Score (macro)\n",
    "    n_jobs=-1                      # Parallele Verarbeitung\n",
    ")\n",
    "\n",
    "# Training mit Hyperparameter-Optimierung\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Beste Parameter und Modell anzeigen\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Beste Parameter:\", best_params)\n",
    "\n",
    "# Finales Modell aus GridSearchCV\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Vorhersagen auf Testdaten\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Bewertung der Ergebnisse\n",
    "print(\"F1-Score: \", f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getrennte Vektorisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatching dimensions along axis 0: {952, 776, 948, 604}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     transformed_features\u001b[38;5;241m.\u001b[39mappend(transformed)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Kombinieren der Features (Sparse-Matrix)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m \u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Splitte die Daten in Trainings- und Testdaten\u001b[39;00m\n\u001b[1;32m     29\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(combined_features, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/scipy/sparse/_construct.py:733\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _block([blocks], \u001b[38;5;28mformat\u001b[39m, dtype)\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_spmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/scipy/sparse/_construct.py:908\u001b[0m, in \u001b[0;36m_block\u001b[0;34m(blocks, format, dtype, return_spmatrix)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28mall\u001b[39m(issparse(b) \u001b[38;5;129;01mand\u001b[39;00m b\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks\u001b[38;5;241m.\u001b[39mflat)\n\u001b[1;32m    905\u001b[0m ):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# stack along columns (axis 1): must have shape (M, 1)\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m [[\u001b[43m_stack_along_minor_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(M)]\n\u001b[1;32m    909\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(blocks, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;66;03m# stack along rows (axis 0):\u001b[39;00m\n",
      "File \u001b[0;32m~/vs-workspace/WiSe24/KI/projekt-git/KI-Python/.venv/lib/python3.12/site-packages/scipy/sparse/_construct.py:648\u001b[0m, in \u001b[0;36m_stack_along_minor_axis\u001b[0;34m(blocks, axis)\u001b[0m\n\u001b[1;32m    646\u001b[0m other_axis_dims \u001b[38;5;241m=\u001b[39m {b\u001b[38;5;241m.\u001b[39mshape[other_axis] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks}\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(other_axis_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMismatching dimensions along axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother_axis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    649\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother_axis_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    650\u001b[0m constant_dim, \u001b[38;5;241m=\u001b[39m other_axis_dims\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# Do the stacking\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatching dimensions along axis 0: {952, 776, 948, 604}"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Kategorien im Datensatz\n",
    "categories = data['group'].unique()\n",
    "\n",
    "# Speichere die Vektorizer und die transformierten Daten\n",
    "tfidf_per_category = {}\n",
    "transformed_features = []\n",
    "\n",
    "max_features = 1000  # Feste Anzahl der Features für alle Kategorien\n",
    "\n",
    "# Trainiere für jede Kategorie einen separaten TF-IDF-Vektorizer\n",
    "for category in categories:\n",
    "    # Dokumente für die Kategorie filtern\n",
    "    docs_in_category = data[data['group'] == category]['text']\n",
    "    \n",
    "    # TF-IDF für die Kategorie trainieren\n",
    "    tfidf = TfidfVectorizer(max_df=0.9, min_df=1, stop_words='english', max_features=max_features)\n",
    "    transformed = tfidf.fit_transform(docs_in_category)\n",
    "    \n",
    "    # Speichern des TF-IDF-Modells pro Kategorie\n",
    "    tfidf_per_category[category] = tfidf\n",
    "    transformed_features.append(transformed)\n",
    "\n",
    "# Kombinieren der Features (Sparse-Matrix)\n",
    "combined_features = hstack(transformed_features)\n",
    "\n",
    "# Splitte die Daten in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, data['group'], test_size=0.2, random_state=42, stratify=data['group'])\n",
    "\n",
    "# Klassifikator (z.B. DecisionTreeClassifier)\n",
    "clf = DecisionTreeClassifier(max_depth=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Bewertung des Modells\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
